\contentsline {chapter}{Acknowledgments}{iii}{dummy.4}%
\contentsline {chapter}{Contribution to Sustainable Development Goals}{v}{dummy.5}%
\contentsline {chapter}{List of Figures}{ix}{dummy.7}%
\contentsline {chapter}{List of Tables}{x}{dummy.9}%
\contentsline {chapter}{Abbreviations}{xi}{dummy.11}%
\contentsline {chapter}{Abstract}{xii}{dummy.13}%
\contentsline {chapter}{\numberline {1}Motivations and Problem Statement (Rooshan Khan)}{1}{chapter.16}%
\contentsline {section}{\numberline {1.1}Proposed Approach}{2}{section.18}%
\contentsline {section}{\numberline {1.2}Project Description}{3}{section.20}%
\contentsline {subsection}{\numberline {1.2.1}Data}{3}{subsection.21}%
\contentsline {subsection}{\numberline {1.2.2}Baseline}{3}{subsection.23}%
\contentsline {subsection}{\numberline {1.2.3}Evaluation}{3}{subsection.24}%
\contentsline {chapter}{\numberline {2}Bidirectional Encoder Representations from Transformers: BERT (Abdul Samad)}{4}{chapter.26}%
\contentsline {section}{\numberline {2.1}Introduction}{4}{section.27}%
\contentsline {subsection}{\numberline {2.1.1}Bidirectional Encoder Representations from Transformers: BERT}{4}{subsection.28}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Introduction}{4}{subsubsection.29}%
\contentsline {section}{\numberline {2.2}Architecture}{4}{section.30}%
\contentsline {section}{\numberline {2.3}Tokenization}{5}{section.32}%
\contentsline {section}{\numberline {2.4}Embedding Layer}{6}{section.34}%
\contentsline {section}{\numberline {2.5}Transformer Encoder Layer}{6}{section.36}%
\contentsline {section}{\numberline {2.6}Pre-training and Fine-tuning BERT Model}{7}{section.42}%
\contentsline {section}{\numberline {2.7}Fine-tuning for Downstream Tasks}{8}{section.44}%
\contentsline {chapter}{\numberline {3}Theoretical Background}{9}{chapter.45}%
\contentsline {section}{\numberline {3.1}SMART: Smoothness Inducing Adversarial Regularization and Bregman Proximal Point Optimization: Rooshan Khan}{9}{section.46}%
\contentsline {subsection}{\numberline {3.1.1}Framework Overview}{9}{subsection.47}%
\contentsline {subsection}{\numberline {3.1.2}Implementation of SMART}{11}{subsection.51}%
\contentsline {section}{\numberline {3.2}Mathematical Foundations and Intuition Behind Supervised SimCSE Contrastive Loss: Abdul Samad}{11}{section.52}%
\contentsline {subsection}{\numberline {3.2.1}Introduction}{11}{subsection.53}%
\contentsline {subsection}{\numberline {3.2.2}Why Contrastive Learning?}{11}{subsection.54}%
\contentsline {subsection}{\numberline {3.2.3}Sentence Embeddings and Similarity}{12}{subsection.55}%
\contentsline {subsection}{\numberline {3.2.4}Supervised SimCSE Using SNLI Triplets}{12}{subsection.59}%
\contentsline {subsubsection}{\numberline {3.2.4.1}Dataset Structure}{12}{subsubsection.60}%
\contentsline {subsubsection}{\numberline {3.2.4.2}Goal}{12}{subsubsection.61}%
\contentsline {subsubsection}{\numberline {3.2.4.3}Mathematical Loss Derivation}{12}{subsubsection.62}%
\contentsline {subsection}{\numberline {3.2.5}Temperature Parameter ($\tau $)}{13}{subsection.63}%
\contentsline {subsection}{\numberline {3.2.6}Key Takeaways}{13}{subsection.64}%
\contentsline {section}{\numberline {3.3}Implementation of Sentence-BERT architecture for Semantic Textual Similarity and Paraphrase Detection: Hussnain Amjad}{13}{section.65}%
\contentsline {subsection}{\numberline {3.3.1}Background and Motivation}{14}{subsection.66}%
\contentsline {subsection}{\numberline {3.3.2}Architecture and Embedding Strategy}{14}{subsection.67}%
\contentsline {subsection}{\numberline {3.3.3}Paraphrase Detection (Classification Task)}{14}{subsection.68}%
\contentsline {subsection}{\numberline {3.3.4}Semantic Textual Similarity (Regression Task)}{15}{subsection.70}%
\contentsline {subsection}{\numberline {3.3.5}Training Details}{17}{subsection.72}%
\contentsline {chapter}{\numberline {4}Experimentation (Rooshan Khan)}{18}{chapter.73}%
\contentsline {section}{\numberline {4.1}Phase 1}{18}{section.74}%
\contentsline {section}{\numberline {4.2}Phase 2}{18}{section.76}%
\contentsline {subsection}{\numberline {4.2.1}Individually fine-tuned BERT models}{20}{subsection.79}%
\contentsline {subsection}{\numberline {4.2.2}Accuracies and Pearson Correlation for Fine-Tuning the same model on Multiple Tasks}{22}{subsection.83}%
\contentsline {subsubsection}{\numberline {4.2.2.1}Sequential Fine-Tuning: SBERT Only}{22}{subsubsection.84}%
\contentsline {subsubsection}{\numberline {4.2.2.2}Sequential Fine-Tuning: SBERT+SMART}{25}{subsubsection.92}%
\contentsline {subsubsection}{\numberline {4.2.2.3}Sequentual Fine-Tuning: SBERT+SimCSE+SMART}{28}{subsubsection.105}%
\contentsline {section}{\numberline {4.3}Analysis of the Results}{28}{section.108}%
\contentsline {chapter}{\numberline {5}Possible Improvements (Areesha Noor)}{30}{chapter.110}%
\vspace {2em}
\contentsline {chapter}{\numberline {A}Implementation of Techniques in Python (Rooshan Khan)}{32}{appendix.111}%
\contentsline {section}{\numberline {A.1}SMART}{32}{section.112}%
\contentsline {subsection}{\numberline {A.1.1}Implementation of methods for predictions}{41}{subsection.410}%
\contentsline {chapter}{\numberline {B}Implementation of Supervised SimCSE in Python (Abdul Samad)}{47}{appendix.557}%
\contentsline {section}{\numberline {B.1}Supervised SimCSE}{47}{section.558}%
\contentsline {subsection}{\numberline {B.1.1}SNLI Dataset Preprocessing}{47}{subsection.559}%
\contentsline {subsection}{\numberline {B.1.2}Triplet Contrastive Loss for SimCSE}{48}{subsection.579}%
\contentsline {subsection}{\numberline {B.1.3}Supervised SimCSE Loss Implementation}{48}{subsection.580}%
\contentsline {subsection}{\numberline {B.1.4}Training Loop for Supervised SimCSE}{50}{subsection.629}%
\contentsline {subsection}{\numberline {B.1.5}SNLI Dataset Preparation}{51}{subsection.661}%
\contentsline {chapter}{\numberline {C}SBERT Implementation for Paraphrase Detection and Semantic Similarity (Hussnain Amjad)}{55}{appendix.785}%
\contentsline {section}{\numberline {C.1}Overview}{55}{section.786}%
\vspace {2em}
\contentsline {chapter}{References}{56}{dummy.787}%
