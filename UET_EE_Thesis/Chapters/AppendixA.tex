% Appendix A

\chapter{Implementation of Techniques in Python (Rooshan Khan)}
\label{AppendixA}
\lhead{Appendix A. \emph{Implementation of Techniques in Python}}

The foundational codebase utilized in our final year project was adapted from materials provided in the Stanford \textit{CS224n: Natural Language Processing with Deep Learning} course~\cite{cs224n2024, amahankali2024}.


\section{SMART}
The theory related to SMART was explained in Section~\ref{sec:SMART_theory}. Now let's talk about how we implemented the SMART algorithm  in python.

The Listing~\ref{lst:sst_SMART} shows the code that trains model on the SST dataset using SMART. It correctly implements the SMART algorithm described in the research paper \textit{SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization} by Jiang et al.~\cite{jiang2019smart}.
Hyperparameters are set manually.
An \texttt{if} condition for $\lambda_s = 0$ or $\mu = 0$ has been added to easily switch between training with SMART and without SMART. We keep $S = 1$ and $T_x = 1$ throughout our training on all tasks. This choice is motivated by computational complexity. Moreover, the authors of the paper themselves used and recommended setting $S = 1$ and $T_x = 1$.
You can see that the \texttt{predict\_sentiment} method is called three times in the loop for every batch. The reason is as follows: it is first needed inside the $T_x$-loop to compute $g_i^{\tilde{}}$. It is then needed a second time outside the $T_x$-loop but inside the $S$-loop, where it is used to compute the logits without noise and the logits with updated noise; these logits are used to calculate $R_s$. Finally, the \texttt{predict\_sentiment} method is called a third time to compute $D_{\text{Breg}}$. In the third case, we use \texttt{model\_copy.predict\_sentiment} because \texttt{model\_copy} has parameters $\tilde{\theta}_t$.

\begin{lstlisting}[language=Python, caption={Sentiment Analysis Training with SMART}, label={lst:sst_SMART}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
# Values of Few hyperparameters

epsilon=1e-5
eps = 1e-8
lambda_s=10
std=1e-5
mean=0
S=1
Tx=1
eta=1e-3
mu=1

# Run for the specified number of epochs.
for epoch in range(args.epochs):
    model.train()
    train_loss = 0
    num_batches = 0
    theta_t=copy.deepcopy(model.state_dict())
    theta_tilde_t=copy.deepcopy(theta_t)
    Beta=0.99
    n=0
    for batch in tqdm(sst_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
        b_ids, b_mask, b_labels = (batch['token_ids'],
                                   batch['attention_mask'], batch['labels'])

        b_ids = b_ids.to(device)
        b_mask = b_mask.to(device)
        b_labels = b_labels.to(device)
        if lambda_s==0 or mu==0:
            theta_tilde_s=copy.deepcopy(theta_t)
            if n==54:
                Beta=0.999
            n+=1
            model_copy = copy.deepcopy(model)
            model_copy.load_state_dict(theta_tilde_t)
            for s in range(0,S):
                noise = torch.randn(b_ids.shape[0], b_ids.shape[1], 768)*std+mean
                noise=noise.to(device)
                for m in range(0,Tx):
                    logits,logits_wn,Embedding_in,Embedding_in_wn = model.predict_sentiment(b_ids,b_mask,noise)
                    P1=F.softmax(logits, dim=1)
                    Q=F.softmax(logits_wn, dim=1)
                    KL_divergence_PQ = P1 * torch.log((P1 + eps) / (Q + eps))
                    KL_divergence_QP = Q * torch.log((Q + eps) / (P1 + eps))
                    ls=KL_divergence_PQ+KL_divergence_QP
                    max_ls,_=torch.max(ls,-1)
                    ls=ls.sum()
                    Rs=torch.sum(max_ls)/max_ls.shape[0]
                    Embedding_in_wn
                    ls.backward()
                    gi=Embedding_in_wn.grad/Embedding_in_wn.shape[0]
                    gi_tilde=gi/(torch.norm(gi,p=float('inf'),dim=(1, 2)).view(gi.shape[0],1,1)+eps)
                    noise = noise.clone() + eta * gi_tilde.clone()
                logits,logits_wn,Embedding_in,Embedding_in_wn = model.predict_sentiment(b_ids,b_mask,noise)
                P1=F.softmax(logits, dim=1)
                Q=F.softmax(logits_wn, dim=1)
                KL_divergence_PQ = P1 * torch.log((P1 + eps) / (Q + eps))
                KL_divergence_QP = Q * torch.log((Q + eps) / (P1 + eps))
                ls=KL_divergence_PQ+KL_divergence_QP
                max_ls,_=torch.max(ls,-1)
                ls=ls.sum()
                Rs=torch.sum(max_ls)/max_ls.shape[0]
                logits2,logits2_wn,Embedding_in_2,Embedding_in_wn_2 = model_copy.predict_sentiment(b_ids,b_mask,noise)
                P2=F.softmax(logits2, dim=1)
                KL_divergence_P1P2 = P1 * torch.log((P1 + eps) / (P2 + eps))
                KL_divergence_P2P1 = P2 * torch.log((P2 + eps) / (P1 + eps))
                ls2=KL_divergence_P1P2+KL_divergence_P2P1
                D_Breg=ls2.sum()/args.batch_size
                optimizer.zero_grad()
                loss=F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size + lambda_s*Rs +mu*D_Breg
                loss.backward()
                optimizer.step()
                theta_tilde_s=copy.deepcopy(model.state_dict())
            theta_t=copy.deepcopy(theta_tilde_s)
            theta_t_tilde = copy.deepcopy(theta_t)
            for key in theta_tilde_s.keys():
                theta_t_tilde[key] = (1 - Beta) * theta_tilde_s[key] + Beta * theta_t_tilde[key]
        else:
            optimizer.zero_grad()
            logits = model.predict_sentiment(b_ids, b_mask)
            loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size
            loss.backward()
            optimizer.step()
        train_loss += loss.item()
        num_batches += 1

    train_loss = train_loss / (num_batches)

    sst_train_acc, sst_train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)
    sst_dev_acc, sst_dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)

    if sst_dev_acc > best_sst_dev_acc:
        best_sst_dev_acc = sst_dev_acc
        save_model(model, optimizer, args, config, args.filepath)

    print(f"Epoch {epoch}: train loss :: {train_loss :.3f}, sst train acc :: {sst_train_acc :.3f}, sst dev acc :: {sst_dev_acc :.3f}")
\end{lstlisting}

Listing~\ref{lst:sts_SMART} shows the code that trains the model on the STS dataset using SMART. This code is similar to that given in Listing~\ref{lst:sst_SMART}, which trains the model for sentiment analysis. There are two differences. The first difference lies in the computation of $l_s$. In this case, $l_s(p, q) = (p - q)^2$, rather than the sum of the KL divergences, i.e., $\text{KL}(P \| Q)$ and $\text{KL}(Q \| P)$. The second difference is that, since there are two inputs (embeddings of all tokens in the input sentence), we need to find and update the noise for both inputs.

\begin{lstlisting}[language=Python, caption={Sematic Textual Similarity Training with SMART}, label={lst:sts_SMART}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
    # Values of Few hyperparameters
    epsilon=1e-5
    eps = 1e-8
    lambda_s=5
    std=1e-5
    mean=0
    S=1
    Tx=1
    eta=1e-3
    mu=1
    # Run for the specified number of epochs.
    for epoch in range(args.epochs//10*2):
        model.train()
        train_loss = 0
        num_batches = 0
        theta_t=copy.deepcopy(model.state_dict())
        theta_tilde_t=copy.deepcopy(theta_t)
        Beta=0.99
        n=0
        for batch in tqdm(sts_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
            b_ids1, b_mask1,b_ids2, b_mask2, b_labels = (batch['token_ids_1'], batch['attention_mask_1'],batch['token_ids_2'],batch['attention_mask_2'],batch['labels'])

            b_ids1 = b_ids1.to(device)
            b_mask1 = b_mask1.to(device)
            b_ids2 = b_ids2.to(device)
            b_mask2 = b_mask2.to(device)
            b_labels=b_labels.to(device)
            if lambda_s!=0 or mu!=0:
                theta_tilde_s=copy.deepcopy(theta_t)

                if n==38:
                    Beta=0.999
                n+=1
                model_copy = copy.deepcopy(model)
                model_copy.load_state_dict(theta_tilde_t)
                for s in range(0,S):
                    noise1 = torch.randn(b_ids1.shape[0], b_ids1.shape[1], 768)*std+mean
                    noise1=noise1.to(device)
                    noise2 = torch.randn(b_ids2.shape[0], b_ids2.shape[1], 768)*std+mean
                    noise2=noise2.to(device)
                    for m in range(0,Tx):
                        logit0,logit0_wn,Embedding_in1,Embedding_in2,Embedding_in1_wn,Embedding_in2_wn = model.predict_similarity(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2)
                        ls=(logit0_wn-logit0)**2
                        max_ls=ls
                        ls=ls.sum()
                        Rs=torch.sum(max_ls)/max_ls.shape[0]
                        Embedding_in1_wn.retain_grad()
                        Embedding_in2_wn.retain_grad()
                        ls.backward()
                        gi1=Embedding_in1_wn.grad/Embedding_in1_wn.shape[0]
                        gi2=Embedding_in2_wn.grad/Embedding_in2_wn.shape[0]
                        gi1_tilde=gi1/(torch.norm(gi1,p=float('inf'),dim=(1, 2)).view(gi1.shape[0],1,1)+eps)
                        gi2_tilde=gi2/(torch.norm(gi2,p=float('inf'),dim=(1, 2)).view(gi2.shape[0],1,1)+eps)
                        noise1 = noise1.clone() + eta * gi1_tilde.clone()
                        noise2 = noise2.clone() + eta * gi2_tilde.clone()
                    logit,logit_wn,Embedding_in1_1,Embedding_in2_1,Embedding_in1_wn_1,Embedding_in2_wn_1 = model.predict_similarity(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2) 
                    ls=(logit-logit_wn)**2
                    max_ls=ls
                    ls=ls.sum()
                    Rs=torch.sum(max_ls)/max_ls.shape[0]
                    logit2,logit2_wn,Embedding_in1_2,Embedding_in2_2,Embedding_in1_wn_2,Embedding_in2_wn_2 = model_copy.predict_similarity(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2)
                    ls2=(logit2-logit)**2
                    D_Breg=ls2.sum()/args.batch_size
                    optimizer.zero_grad()
                    loss=F.mse_loss(logit, b_labels.view(-1).float(), reduction='sum') / args.batch_size + lambda_s*Rs +mu*D_Breg
                    loss.backward()
                    optimizer.step()
                    theta_tilde_s=copy.deepcopy(model.state_dict())
                theta_t=copy.deepcopy(theta_tilde_s)
                theta_t_tilde = copy.deepcopy(theta_t)
                for key in theta_tilde_s.keys():
                    theta_t_tilde[key] = (1 - Beta) * theta_tilde_s[key] + Beta * theta_t_tilde[key]
            else:
                optimizer.zero_grad()
                logit = model.predict_similarity(b_ids1, b_mask1, b_ids2, b_mask2)
                loss = F.mse_loss(logit, b_labels.view(-1).float(), reduction='sum') / args.batch_size
                loss.backward()
                optimizer.step()
            train_loss += loss.item()
            num_batches += 1

        train_loss = train_loss / (num_batches)

        train_sts_corr= model_eval_sts(sts_train_dataloader, model, device)
        dev_sts_corr= model_eval_sts(sts_dev_dataloader, model, device)

        if dev_sts_corr > best_sts_corr:
            best_sts_corr = dev_sts_corr
            save_model(model, optimizer, args, config, args.filepath)

        print(f"Epoch {epoch}: train loss :: {train_loss :.3f}, train_sts_corr :: {train_sts_corr :.3f}, dev_sts_corr :: {dev_sts_corr :.3f}")

\end{lstlisting}

Listing~\ref{lst:QQP_SMART} shows the code that trains the model on the Quora Question Pairs (QQP) dataset using SMART. This code is similar to that given in Listing~\ref{lst:sst_SMART}. Although QQP is also a classification task like SST, it involves binary classification. There is a single logit output. Thus, we cannot apply the softmax function to the logit. Instead, we first compute $\text{sigmoid}(\mathrm{logit})$ and $\text{sigmoid}(\mathrm{logit\_wn})$, treating these values as the probabilities of being a paraphrase. The probabilities of not being a paraphrase are computed as $1 - \text{sigmoid}(\mathrm{logit})$ and $1 - \text{sigmoid}(\mathrm{logit\_wn})$.



\begin{lstlisting}[language=Python, caption={Paraphrase Detection Training with SMART}, label={lst:QQP_SMART}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
    # Values of Few hyperparameters
    epsilon=1e-5
    eps = 1e-8
    lambda_s=5
    std=1e-5
    mean=0
    S=1
    Tx=1
    eta=1e-3
    mu=1
    # Run for the specified number of epochs.
    for epoch in range((args.epochs//10)*3):
        model.train()
        train_loss = 0
        num_batches = 0
        theta_t=copy.deepcopy(model.state_dict())
        theta_tilde_t=copy.deepcopy(theta_t)
        Beta=0.99
        n=0
        for batch in tqdm(para_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
            b_ids1, b_mask1,b_ids2, b_mask2,b_labels= (batch['token_ids_1'], batch['attention_mask_1'],batch['token_ids_2'], batch['attention_mask_2'],batch['labels'])


            b_ids1 = b_ids1.to(device)
            b_mask1 = b_mask1.to(device)
            b_ids2 = b_ids2.to(device)
            b_mask2 = b_mask2.to(device)
            b_labels=b_labels.to(device)

            if lambda_s!=0 or mu!=0:
                theta_tilde_s=copy.deepcopy(theta_t)
                if n==1769:
                    Beta=0.999
                n+=1
                model_copy = copy.deepcopy(model)
                model_copy.load_state_dict(theta_tilde_t)
                for s in range(0,S):
                    noise1 = torch.randn(b_ids1.shape[0], b_ids1.shape[1], 768)*std+mean
                    noise1=noise1.to(device)
                    noise2 = torch.randn(b_ids2.shape[0], b_ids2.shape[1], 768)*std+mean
                    noise2=noise2.to(device)
                    for m in range(0,Tx):
                        logit,logit_wn,_,_,Embedding_in1_wn,Embedding_in2_wn = model.predict_paraphrase(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2)
                        prob = torch.sigmoid(logit)    # shape: (batch_size, 1)
                        prob_wn = torch.sigmoid(logit_wn)  # with noise
                        P1 = torch.cat([1 - prob, prob], dim=1)  # [P(not paraphrase), P(paraphrase)]
                        Q = torch.cat([1 - prob_wn, prob_wn], dim=1)

                        KL_divergence_PQ = P1 * torch.log((P1 + eps) / (Q + eps))
                        KL_divergence_QP = Q * torch.log((Q + eps) / (P1 + eps))
                        ls=KL_divergence_PQ+KL_divergence_QP
                        ls=ls.sum()
                        ls.backward()
                        gi1_tilde=Embedding_in1_wn.grad/Embedding_in1_wn.shape[0]
                        gi2_tilde=Embedding_in2_wn.grad/Embedding_in2_wn.shape[0]
                        gi1_tilde/=(torch.norm(gi1_tilde,p=float('inf'),dim=(1, 2)).view(gi1_tilde.shape[0],1,1)+eps)
                        gi2_tilde/=(torch.norm(gi2_tilde,p=float('inf'),dim=(1, 2)).view(gi2_tilde.shape[0],1,1)+eps)
                        noise1 += eta * gi1_tilde
                        noise2 += eta * gi2_tilde
                    logit,logit_wn,*rest = model.predict_paraphrase(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2) 
                    prob = torch.sigmoid(logit)    # shape: (batch_size, 1)
                    prob_wn = torch.sigmoid(logit_wn)  # with noise
                    P1 = torch.cat([1 - prob, prob], dim=1)  # [P(not paraphrase), P(paraphrase)]
                    Q = torch.cat([1 - prob_wn, prob_wn], dim=1)
                    KL_divergence_PQ = P1 * torch.log((P1 + eps) / (Q + eps))
                    KL_divergence_QP = Q * torch.log((Q + eps) / (P1 + eps))
                    ls=KL_divergence_PQ+KL_divergence_QP
                    max_ls,_=torch.max(ls,-1)
                    ls=ls.sum()
                    Rs=torch.sum(max_ls)/max_ls.shape[0]
                    logit2,*rest = model_copy.predict_paraphrase(b_ids1, b_mask1, b_ids2, b_mask2, noise1, noise2)
                    prob2 = torch.sigmoid(logit2)    # shape: (batch_size, 1)
                    P2 = torch.cat([1 - prob2, prob2], dim=1)  # [P(not paraphrase), P(paraphrase)]
                    KL_divergence_P1P2 = P1 * torch.log((P1 + eps) / (P2 + eps))
                    KL_divergence_P2P1 = P2 * torch.log((P2 + eps) / (P1 + eps))
                    ls2=KL_divergence_P1P2+KL_divergence_P2P1
                    D_Breg=ls2.sum()/args.batch_size
                    optimizer.zero_grad()
                    loss=model.paraphrase_simcse_loss(b_ids1, b_mask1,b_ids2, b_mask2,b_labels,pooling='mean',temperature=0.05) + lambda_s*Rs + mu*D_Breg
                    loss.backward()
                    optimizer.step()
                    theta_tilde_s=copy.deepcopy(model.state_dict())
                theta_t=copy.deepcopy(theta_tilde_s)
                theta_t_tilde = copy.deepcopy(theta_t)
                for key in theta_tilde_s.keys():
                    theta_t_tilde[key] = (1 - Beta) * theta_tilde_s[key] + Beta * theta_t_tilde[key]
            else:
                optimizer.zero_grad()
                logit = model.predict_similarity(b_ids1, b_mask1, b_ids2, b_mask2)
                loss = F.binary_cross_entropy_with_logits(logit.squeeze(), b_labels.view(-1).float(), reduction='sum') / args.batch_size
                loss.backward()
                optimizer.step()
            train_loss += loss.item()
            num_batches += 1
        train_loss = train_loss / (num_batches)


        paraphrase_train_acc = model_eval_paraphrase(para_train_dataloader, model, device)
        paraphrase_dev_acc = model_eval_paraphrase(para_dev_dataloader, model, device)

        if paraphrase_dev_acc > best_paraphrase_dev_acc:
            best_paraphrase_dev_acc = paraphrase_dev_acc
            save_model(model, optimizer, args, config, args.filepath)

        print(f"Epoch {epoch}: train loss :: {train_loss :.3f}, paraphrase train acc :: {paraphrase_train_acc :.3f}, paraphrase dev acc :: {paraphrase_dev_acc :.3f}")

\end{lstlisting}

\subsection{Implementation of methods for predictions}

Look at the codes shown in Listing~\ref{lst:predict_sentiment}, Listing~\ref{lst:predict_similarity}, and Listing~\ref{lst:predict_paraphrase}. All of these use mean pooling as the default. The \texttt{if} condition checking whether \texttt{noise} is a tensor or not was added for the implementation of SMART during training. The default value of \texttt{noise} is $0$, which is an integer and not a tensor. This allows the same \texttt{predict} method to be called during both training and evaluation. During evaluation, \texttt{noise} remains an integer, so SMART is not applied. To achieve this behavior, we also modified the \texttt{forward} method of \texttt{BERTModel}, as shown in Listing~\ref{lst:Forw_method_BERTModel}. The dictionary returned by this method differs between training with SMART and evaluation. Another important detail in Listings~\ref{lst:predict_sentiment}, \ref{lst:predict_similarity}, and \ref{lst:predict_paraphrase} is that mean pooling is not performed using \texttt{torch.mean(tensor, dim=-1)}. Upon inspecting \texttt{b\_ids}, we observed the presence of padding tokens. If we had used simple mean pooling with \texttt{torch.mean}, it would have incorrectly averaged the embeddings of all tokens, including the padding tokens, leading to distorted representations. Instead, we applied mean pooling using the attention mask to sum only the embeddings corresponding to the \texttt{[CLS]} token, \texttt{[SEP]} token, and the actual word tokens between them, while ignoring the padding tokens. The same masked mean pooling approach is consistently used across all the \texttt{predict} methods. 

\begin{lstlisting}[language=Python, caption={Changed forward method in BERTModel}, label={lst:Forw_method_BERTModel}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
    def forward(self, input_ids,attention_mask,perturbation=0,epsilon=1e-5):
        """
        input_ids: [batch_size, seq_len], seq_len is the max length of the batch
        attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens
        """
        # Get the embedding for each input token.
        embedding_output = self.embed(input_ids=input_ids).requires_grad_()
        # Feed to a transformer (a stack of BertLayers).
        sequence_output = self.encode(embedding_output, attention_mask=attention_mask)

        # Get cls token hidden state.
        first_tk = sequence_output[:, 0]
        first_tk = self.pooler_dense(first_tk)
        first_tk = self.pooler_af(first_tk)
        if isinstance(perturbation, torch.Tensor):
            norm = (torch.norm(perturbation, p=float('inf'),dim=(1, 2))).view(perturbation.shape[0], 1, 1)
            perturbation=perturbation*epsilon/(norm+1e-8)
            embedding_output_with_noise=embedding_output+perturbation
            embedding_output_with_noise=embedding_output_with_noise.detach().requires_grad_()
            sequence_output_n = self.encode(embedding_output_with_noise, attention_mask=attention_mask)
            # Get cls token hidden state for embedding_output_with_noise.
            first_tk_n = sequence_output_n[:, 0]
            first_tk_n = self.pooler_dense(first_tk_n)
            first_tk_n = self.pooler_af(first_tk_n)
            return {'last_hidden_state': sequence_output, 'pooler_output': first_tk,'embedding_output': embedding_output,'last_hidden_state_wn': sequence_output_n, 'pooler_output_wn': first_tk_n,'embedding_output_with_noise': embedding_output_with_noise}
        return {'last_hidden_state': sequence_output, 'pooler_output': first_tk}
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={predict sentiment method in Multitask BERT class}, label={lst:predict_sentiment}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
def predict_sentiment(self, input_ids, attention_mask,noise=0,pooling='mean'):
    '''Given a batch of sentences, outputs logits for classifying sentiment.
    There are 5 sentiment classes:
    (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)
    Thus, your output should contain 5 logits for each sentence.
    '''
    ### TODO
    embedding_output=self.forward(input_ids, attention_mask,noise=noise)
    if pooling=='mean':
        hidden_states=embedding_output['last_hidden_state']
        mask=attention_mask.unsqueeze(-1).expand(-1, -1, hidden_states.shape[2]).float()
        den=torch.sum(attention_mask,dim=1).unsqueeze(-1).expand(-1, hidden_states.shape[2])+1e-9
        vector = torch.sum(mask*(embedding_output['last_hidden_state']), dim=1)/den
    elif pooling=='cls':
        vector=embedding_output['pooler_output']
    Dropout_layer_output=self.dropout_layer(vector)
    logits=self.sentiment_classifier_layer(Dropout_layer_output) 
    if isinstance(noise, torch.Tensor):
        if pooling=='mean':
            vector_wn = torch.sum(mask*(embedding_output['last_hidden_state_wn']), dim=1)/den
        elif pooling=='cls':
            vector_wn=embedding_output['pooler_output_wn']
        Dropout_layer_output_wn=self.dropout_layer(vector_wn)
        logits_wn=self.sentiment_classifier_layer(Dropout_layer_output_wn)
        Embedding_in=embedding_output['embedding_output']
        Embedding_in_wn=embedding_output['embedding_output_with_noise']

        return logits,logits_wn,Embedding_in,Embedding_in_wn
    return logits
\end{lstlisting}


\begin{lstlisting}[language=Python, caption={predict similarity method in Multitask BERT class}, label={lst:predict_similarity}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
def predict_similarity(self,
                        input_ids_1, attention_mask_1,
                        input_ids_2, attention_mask_2,noise1=0,noise2=0,pooling='mean'):
    '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.
    Note that your output should be unnormalized (a logit).
    '''
    ### TODO
    embedding_output1=self.forward(input_ids_1, attention_mask_1,noise1)
    embedding_output2=self.forward(input_ids_2, attention_mask_2,noise2)
    if pooling == 'mean':
        hidden_states1=embedding_output1['last_hidden_state']
        hidden_states2=embedding_output2['last_hidden_state']
        mask1=attention_mask_1.unsqueeze(-1).expand(-1, -1, hidden_states1.shape[2]).float()
        mask2=attention_mask_2.unsqueeze(-1).expand(-1, -1, hidden_states1.shape[2]).float()
        den1=torch.sum(attention_mask_1,dim=1).unsqueeze(-1).expand(-1, hidden_states1.shape[2])+1e-9
        den2=torch.sum(attention_mask_2,dim=1).unsqueeze(-1).expand(-1, hidden_states1.shape[2])+1e-9

        vector1 = torch.sum(mask1*(embedding_output1['last_hidden_state']), dim=1)/den1
        vector2 = torch.sum(mask2*(embedding_output2['last_hidden_state']), dim=1)/den2
    if pooling=='cls':
        vector1=embedding_output1['pooler_output']
        vector2=embedding_output2['pooler_output']
    Dropout_layer_output1=self.dropout_layer(vector1)
    Dropout_layer_output2=self.dropout_layer(vector2)
    Cosine_similarity=(torch.sum(Dropout_layer_output1*Dropout_layer_output2,dim=1)/((torch.norm(Dropout_layer_output1,dim=1)*torch.norm(Dropout_layer_output2,dim=1)))+1+1e-8)*2.5
    if isinstance(noise1, torch.Tensor) and isinstance(noise2, torch.Tensor):
        if pooling=='mean':
            vector1_wn = torch.sum(mask1*(embedding_output1['last_hidden_state_wn']), dim=1)/den1
            vector2_wn = torch.sum(mask2*(embedding_output2['last_hidden_state_wn']), dim=1)/den2
        if pooling=='cls':
            vector1_wn=embedding_output1['pooler_output_wn']
            vector2_wn=embedding_output2['pooler_output_wn']        
        Dropout_layer_output1_wn=self.dropout_layer(vector1_wn)
        Dropout_layer_output2_wn=self.dropout_layer(vector2_wn)
        Cosine_similarity_wn=(torch.sum(Dropout_layer_output1_wn*Dropout_layer_output2_wn,dim=1)/((torch.norm(Dropout_layer_output1_wn,dim=1)*torch.norm(Dropout_layer_output2_wn,dim=1)))+1+1e-8)*2.5
        Embedding_in1=embedding_output1['embedding_output']
        Embedding_in2=embedding_output2['embedding_output'] 
        Embedding_in1_wn=embedding_output1['embedding_output_with_noise']
        Embedding_in2_wn=embedding_output2['embedding_output_with_noise']
        return Cosine_similarity,Cosine_similarity_wn,Embedding_in1,Embedding_in2,Embedding_in1_wn,Embedding_in2_wn
    return Cosine_similarity
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={predict paraphrase method in Multitask BERT class}, label={lst:predict_paraphrase}, basicstyle=\ttfamily\small, keywordstyle=\color{blue}, commentstyle=\color{green}, stringstyle=\color{red},linewidth=\textwidth]
def predict_paraphrase(self,
                        input_ids_1, attention_mask_1,
                        input_ids_2, attention_mask_2,noise1=0,noise2=0,pooling='mean'):
    '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.
    Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function
    during evaluation.
    '''
    ### TODO
    embedding_output1=self.forward(input_ids_1, attention_mask_1,noise1)
    embedding_output2=self.forward(input_ids_2, attention_mask_2,noise2)
    if pooling=='mean':
        hidden_states1=embedding_output1['last_hidden_state']
        hidden_states2=embedding_output2['last_hidden_state']
        mask1=attention_mask_1.unsqueeze(-1).expand(-1, -1, hidden_states1.shape[2]).float()
        mask2=attention_mask_2.unsqueeze(-1).expand(-1, -1, hidden_states1.shape[2]).float()
        den1=torch.sum(attention_mask_1,dim=1).unsqueeze(-1).expand(-1, hidden_states1.shape[2])+1e-9
        den2=torch.sum(attention_mask_2,dim=1).unsqueeze(-1).expand(-1, hidden_states1.shape[2])+1e-9

        vector1 = torch.sum(mask1*(embedding_output1['last_hidden_state']), dim=1)/den1
        vector2 = torch.sum(mask2*(embedding_output2['last_hidden_state']), dim=1)/den2
    elif pooling=='cls':
        vector1=embedding_output1['pooler_output']
        vector2=embedding_output2['pooler_output']
    Dropout_layer_output1=self.dropout_layer(vector1)
    Dropout_layer_output2=self.dropout_layer(vector2)

    Concatenated_result=torch.concat((Dropout_layer_output1,Dropout_layer_output2,abs(Dropout_layer_output1-Dropout_layer_output2)),dim=1)
    logit=self.paraphrase_classifier_layer(Concatenated_result)
    if isinstance(noise1, torch.Tensor) and isinstance(noise2, torch.Tensor):
        #mean
        if pooling=='mean':
            vector1_wn = torch.sum(mask1*(embedding_output1['last_hidden_state_wn']), dim=1)/den1
            vector2_wn = torch.sum(mask2*(embedding_output2['last_hidden_state_wn']), dim=1)/den2
        if pooling=='cls':
            vector1_wn=embedding_output1['pooler_output_wn']
            vector2_wn=embedding_output2['pooler_output_wn']        
        Dropout_layer_output1_wn=self.dropout_layer(vector1_wn)
        Dropout_layer_output2_wn=self.dropout_layer(vector2_wn)
        Concatenated_result=torch.concat((Dropout_layer_output1_wn,Dropout_layer_output2_wn,abs(Dropout_layer_output1_wn-Dropout_layer_output2_wn)),dim=1)
        logit_wn=self.paraphrase_classifier_layer(Concatenated_result)
        Embedding_in1=embedding_output1['embedding_output']
        Embedding_in2=embedding_output2['embedding_output']
        Embedding_in1_wn=embedding_output1['embedding_output_with_noise']
        Embedding_in2_wn=embedding_output2['embedding_output_with_noise']
        return logit,logit_wn,Embedding_in1,Embedding_in2,Embedding_in1_wn,Embedding_in2_wn
    return logit
\end{lstlisting}

