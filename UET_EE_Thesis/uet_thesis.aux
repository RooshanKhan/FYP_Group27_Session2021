\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{iii}{dummy.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Contribution to Sustainable Development Goals}{v}{dummy.5}\protected@file@percent }
\citation{reimers2019sentence}
\citation{reimers2019sentence}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{dummy.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{dummy.9}\protected@file@percent }
\gdef \LT@i {\LT@entry 
    {1}{57.62473pt}\LT@entry 
    {1}{410.33858pt}}
\@writefile{toc}{\contentsline {chapter}{Abbreviations}{xi}{dummy.11}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{xii}{dummy.13}\protected@file@percent }
\citation{devlin2018bert}
\citation{jiang2019smart}
\citation{jiang2019smart}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Motivations and Problem Statement (Rooshan Khan)}{1}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter1}{{1}{1}{Motivations and Problem Statement (Rooshan Khan)}{chapter.16}{}}
\citation{reimers2019sentence}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Tasks and Ratings for simple BERT model without finetuning}}{2}{table.caption.17}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tasks_ratings1}{{1.1}{2}{Tasks and Ratings for simple BERT model without finetuning}{table.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Proposed Approach}{2}{section.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Proposed Model}}{3}{figure.caption.19}\protected@file@percent }
\newlabel{PM}{{1.1}{3}{Proposed Model}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Project Description}{3}{section.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Data}{3}{subsection.21}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Dataset splits used for training, validation, and testing.}}{3}{table.caption.22}\protected@file@percent }
\newlabel{tab:dataset_splits}{{1.2}{3}{Dataset splits used for training, validation, and testing}{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Baseline}{3}{subsection.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Evaluation}{3}{subsection.24}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Evaluation metrics for the tasks}}{3}{table.caption.25}\protected@file@percent }
\newlabel{tasks_ratings0}{{1.3}{3}{Evaluation metrics for the tasks}{table.caption.25}{}}
\citation{devlin2018bert}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bidirectional Encoder Representations from Transformers: BERT (Abdul Samad)}{4}{chapter.26}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{4}{Bidirectional Encoder Representations from Transformers: BERT (Abdul Samad)}{chapter.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{4}{section.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Bidirectional Encoder Representations from Transformers: BERT}{4}{subsection.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Introduction}{4}{subsubsection.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Architecture}{4}{section.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces BERT Architecture. Image sourced from \url  {https://wikidocs.net/book/1}.}}{5}{figure.caption.31}\protected@file@percent }
\newlabel{4}{{2.1}{5}{BERT Architecture. Image sourced from \url {https://wikidocs.net/book/1}}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Tokenization}{5}{section.32}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces WordPiece Tokenization}}{6}{table.caption.33}\protected@file@percent }
\newlabel{tab:wordpiece}{{2.1}{6}{WordPiece Tokenization}{table.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Embedding Layer}{6}{section.34}\protected@file@percent }
\newlabel{eqn11}{{2.1}{6}{Embedding Layer}{equation.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Transformer Encoder Layer}{6}{section.36}\protected@file@percent }
\newlabel{eqn12}{{2.2}{6}{Transformer Encoder Layer}{equation.37}{}}
\newlabel{eqn13}{{2.3}{6}{Transformer Encoder Layer}{equation.38}{}}
\newlabel{eqn14}{{2.4}{6}{Transformer Encoder Layer}{equation.39}{}}
\newlabel{eqn15}{{2.5}{7}{Transformer Encoder Layer}{equation.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Pre-training and Fine-tuning BERT Model}{7}{section.42}\protected@file@percent }
\citation{rajpurkar2016squad}
\newlabel{eqn16}{{2.6}{8}{Pre-training and Fine-tuning BERT Model}{equation.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Fine-tuning for Downstream Tasks}{8}{section.44}\protected@file@percent }
\citation{jiang2019smart}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{9}{chapter.45}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{9}{Theoretical Background}{chapter.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}SMART: Smoothness Inducing Adversarial Regularization and Bregman Proximal Point Optimization: Rooshan Khan}{9}{section.46}\protected@file@percent }
\newlabel{sec:SMART_theory}{{3.1}{9}{SMART: Smoothness Inducing Adversarial Regularization and Bregman Proximal Point Optimization: Rooshan Khan}{section.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Framework Overview}{9}{subsection.47}\protected@file@percent }
\citation{gao2021simcse}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Implementation of SMART}{11}{subsection.51}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Mathematical Foundations and Intuition Behind Supervised SimCSE Contrastive Loss: Abdul Samad}{11}{section.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction}{11}{subsection.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Why Contrastive Learning?}{11}{subsection.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Sentence Embeddings and Similarity}{12}{subsection.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Supervised SimCSE Using SNLI Triplets}{12}{subsection.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.1}Dataset Structure}{12}{subsubsection.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.2}Goal}{12}{subsubsection.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.3}Mathematical Loss Derivation}{12}{subsubsection.62}\protected@file@percent }
\citation{reimers2019sentence}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Temperature Parameter ($\tau $)}{13}{subsection.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Key Takeaways}{13}{subsection.64}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Implementation of Sentence-BERT architecture for Semantic Textual Similarity and Paraphrase Detection: Hussnain Amjad}{13}{section.65}\protected@file@percent }
\citation{reimers2019sentence}
\citation{reimers2019sentence}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Background and Motivation}{14}{subsection.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Architecture and Embedding Strategy}{14}{subsection.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Paraphrase Detection (Classification Task)}{14}{subsection.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces SBERT Implementation Architecture for Paraphrase Detection. Referenced from \textit  {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks} by Reimers and Gurevych~\cite  {reimers2019sentence}}}{15}{figure.caption.69}\protected@file@percent }
\newlabel{fig:sbert_pd_architecture}{{3.1}{15}{SBERT Implementation Architecture for Paraphrase Detection. Referenced from \textit {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks} by Reimers and Gurevych~\cite {reimers2019sentence}}{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Semantic Textual Similarity (Regression Task)}{15}{subsection.70}\protected@file@percent }
\citation{reimers2019sentence}
\citation{reimers2019sentence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces SBERT Implementation Architecture for Semantic Textual Similarity. Referenced from \textit  {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks} by Reimers and Gurevych~\cite  {reimers2019sentence}}}{16}{figure.caption.71}\protected@file@percent }
\newlabel{fig:sbert_sts_architecture}{{3.2}{16}{SBERT Implementation Architecture for Semantic Textual Similarity. Referenced from \textit {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks} by Reimers and Gurevych~\cite {reimers2019sentence}}{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Training Details}{17}{subsection.72}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimentation (Rooshan Khan)}{18}{chapter.73}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{18}{Experimentation (Rooshan Khan)}{chapter.73}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Phase 1}{18}{section.74}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Quantitative Result of Phase 1. Accuracy comparison of BERT on SST and CFIMDB.}}{18}{table.caption.75}\protected@file@percent }
\newlabel{tab:phase1_results}{{4.1}{18}{Quantitative Result of Phase 1. Accuracy comparison of BERT on SST and CFIMDB}{table.caption.75}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Phase 2}{18}{section.76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Proposed Model}}{19}{figure.caption.77}\protected@file@percent }
\newlabel{fig:Proposed Model}{{4.1}{19}{Proposed Model}{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Final Model Architecture}}{20}{figure.caption.78}\protected@file@percent }
\newlabel{fig:Final_Model_Architecture}{{4.2}{20}{Final Model Architecture}{figure.caption.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Individually fine-tuned BERT models}{20}{subsection.79}\protected@file@percent }
\newlabel{subsec:Individually fine-tuned BERT models}{{4.2.1}{20}{Individually fine-tuned BERT models}{subsection.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces SST train vs dev accuracy over 10 epochs}}{21}{figure.caption.80}\protected@file@percent }
\newlabel{fig:sst_accuracy}{{4.3}{21}{SST train vs dev accuracy over 10 epochs}{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces STS train vs dev correlation over 10 epochs}}{21}{figure.caption.81}\protected@file@percent }
\newlabel{fig:sts_corr}{{4.4}{21}{STS train vs dev correlation over 10 epochs}{figure.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Best performance of separate models fine-tuned individually for each task}}{22}{table.caption.82}\protected@file@percent }
\newlabel{tab:smart_per_task}{{4.2}{22}{Best performance of separate models fine-tuned individually for each task}{table.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Accuracies and Pearson Correlation for Fine-Tuning the same model on Multiple Tasks}{22}{subsection.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.1}Sequential Fine-Tuning: SBERT Only}{22}{subsubsection.84}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Development set performance before fine-tuning}}{22}{table.caption.85}\protected@file@percent }
\newlabel{tab:pre_finetuning_metrics}{{4.3}{22}{Development set performance before fine-tuning}{table.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Train vs Dev Accuracy for Paraphrase Detection using SBERT Only (3 epochs)}}{23}{figure.caption.86}\protected@file@percent }
\newlabel{fig:paraphrase_acc_plot}{{4.5}{23}{Train vs Dev Accuracy for Paraphrase Detection using SBERT Only (3 epochs)}{figure.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Development set performance after fine-tuning on Paraphrase Detection for 3 epochs}}{23}{table.caption.87}\protected@file@percent }
\newlabel{tab:post_finetuning_paraphrase_metrics}{{4.4}{23}{Development set performance after fine-tuning on Paraphrase Detection for 3 epochs}{table.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Train vs Dev Correlation for Semantic Textual Similarity using SBERT Only (10 epochs)}}{23}{figure.caption.88}\protected@file@percent }
\newlabel{fig:sts_corr_plot}{{4.6}{23}{Train vs Dev Correlation for Semantic Textual Similarity using SBERT Only (10 epochs)}{figure.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Development set performance after fine-tuning on the STS task}}{24}{table.caption.89}\protected@file@percent }
\newlabel{tab:post_finetuning_sts_metrics}{{4.5}{24}{Development set performance after fine-tuning on the STS task}{table.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Train vs Dev Accuracy for Sentiment Analysis using SBERT Only (10 epochs)}}{24}{figure.caption.90}\protected@file@percent }
\newlabel{fig:sst_acc_plot}{{4.7}{24}{Train vs Dev Accuracy for Sentiment Analysis using SBERT Only (10 epochs)}{figure.caption.90}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Development set performance after fine-tuning for 10 epochs on SST}}{24}{table.caption.91}\protected@file@percent }
\newlabel{tab:post_finetuning_metrics}{{4.6}{24}{Development set performance after fine-tuning for 10 epochs on SST}{table.caption.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.2}Sequential Fine-Tuning: SBERT+SMART}{25}{subsubsection.92}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Development set performance after fine-tuning on Paraphrase Detection for 1 epoch}}{25}{table.caption.93}\protected@file@percent }
\newlabel{tab:post_finetuning_metrics_SS}{{4.7}{25}{Development set performance after fine-tuning on Paraphrase Detection for 1 epoch}{table.caption.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Train vs Dev Pearson Correlation for STS using SMART SBERT (10 epochs)}}{25}{figure.caption.94}\protected@file@percent }
\newlabel{fig:SS_sts_corr}{{4.8}{25}{Train vs Dev Pearson Correlation for STS using SMART SBERT (10 epochs)}{figure.caption.94}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Development set performance after fine-tuning for 10 epochs on STS}}{26}{table.caption.95}\protected@file@percent }
\newlabel{tab:post_finetuning_metrics_SS2}{{4.8}{26}{Development set performance after fine-tuning for 10 epochs on STS}{table.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Train vs Dev Accuracy for SST using SMART SBERT (10 epochs)}}{26}{figure.caption.96}\protected@file@percent }
\newlabel{fig:SS_sst_acc}{{4.9}{26}{Train vs Dev Accuracy for SST using SMART SBERT (10 epochs)}{figure.caption.96}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Development set performance after fine-tuning for 10 epochs on SST}}{26}{table.caption.97}\protected@file@percent }
\newlabel{tab:post_finetuning_metrics_SS3}{{4.9}{26}{Development set performance after fine-tuning for 10 epochs on SST}{table.caption.97}{}}
\citation{gao2021simcse}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Updated Training Procedure}}{27}{figure.caption.102}\protected@file@percent }
\newlabel{fig:Training_Procedure}{{4.10}{27}{Updated Training Procedure}{figure.caption.102}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Evaluation metrics for each task (SMART+SBERT)}}{27}{table.caption.103}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Evaluation metrics for each task (SBERT Only)}}{27}{table.caption.104}\protected@file@percent }
\citation{paperswithcode}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.3}Sequentual Fine-Tuning: SBERT+SimCSE+SMART}{28}{subsubsection.105}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Evaluation metrics for each task}}{28}{table.caption.106}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces Evaluation metrics for each task}}{28}{table.caption.107}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Analysis of the Results}{28}{section.108}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.14}{\ignorespaces State-of-the-art BERT-base results for each task}}{29}{table.caption.109}\protected@file@percent }
\newlabel{tab:bert-sota-clean}{{4.14}{29}{State-of-the-art BERT-base results for each task}{table.caption.109}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Possible Improvements (Areesha Noor)}{30}{chapter.110}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter6}{{5}{30}{Possible Improvements (Areesha Noor)}{chapter.110}{}}
\@writefile{toc}{\vspace  {2em}}
\citation{cs224n2024,amahankali2024}
\citation{jiang2019smart}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Implementation of Techniques in Python (Rooshan Khan)}{32}{appendix.111}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{AppendixA}{{A}{32}{Implementation of Techniques in Python (Rooshan Khan)}{appendix.111}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}SMART}{32}{section.112}\protected@file@percent }
\newlabel{lst:sst_SMART}{{A.1}{32}{Sentiment Analysis Training with SMART}{lstlisting.113}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}{\ignorespaces Sentiment Analysis Training with SMART}}{32}{lstlisting.113}\protected@file@percent }
\newlabel{lst:sts_SMART}{{A.2}{35}{Sematic Textual Similarity Training with SMART}{lstlisting.210}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.2}{\ignorespaces Sematic Textual Similarity Training with SMART}}{35}{lstlisting.210}\protected@file@percent }
\newlabel{lst:QQP_SMART}{{A.3}{38}{Paraphrase Detection Training with SMART}{lstlisting.303}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.3}{\ignorespaces Paraphrase Detection Training with SMART}}{38}{lstlisting.303}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Implementation of methods for predictions}{41}{subsection.410}\protected@file@percent }
\newlabel{lst:Forw_method_BERTModel}{{A.4}{42}{Changed forward method in BERTModel}{lstlisting.411}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.4}{\ignorespaces Changed forward method in BERTModel}}{42}{lstlisting.411}\protected@file@percent }
\newlabel{lst:predict_sentiment}{{A.5}{42}{predict sentiment method in Multitask BERT class}{lstlisting.438}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.5}{\ignorespaces predict sentiment method in Multitask BERT class}}{42}{lstlisting.438}\protected@file@percent }
\newlabel{lst:predict_similarity}{{A.6}{43}{predict similarity method in Multitask BERT class}{lstlisting.468}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.6}{\ignorespaces predict similarity method in Multitask BERT class}}{43}{lstlisting.468}\protected@file@percent }
\newlabel{lst:predict_paraphrase}{{A.7}{45}{predict paraphrase method in Multitask BERT class}{lstlisting.510}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.7}{\ignorespaces predict paraphrase method in Multitask BERT class}}{45}{lstlisting.510}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Implementation of Supervised SimCSE in Python (Abdul Samad)}{47}{appendix.557}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{AppendixB}{{B}{47}{Implementation of Supervised SimCSE in Python (Abdul Samad)}{appendix.557}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Supervised SimCSE}{47}{section.558}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}SNLI Dataset Preprocessing}{47}{subsection.559}\protected@file@percent }
\newlabel{lst:snli_preprocess}{{B.1}{47}{Preprocessing SNLI Dataset}{lstlisting.560}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.1}{\ignorespaces Preprocessing SNLI Dataset}}{47}{lstlisting.560}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}Triplet Contrastive Loss for SimCSE}{48}{subsection.579}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Supervised SimCSE Loss Implementation}{48}{subsection.580}\protected@file@percent }
\newlabel{lst:simcse_loss}{{B.2}{48}{Supervised SimCSE Loss}{lstlisting.581}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.2}{\ignorespaces Supervised SimCSE Loss}}{48}{lstlisting.581}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.4}Training Loop for Supervised SimCSE}{50}{subsection.629}\protected@file@percent }
\newlabel{lst:simcse_train}{{B.3}{50}{Training SimCSE with Triplet SNLI Data}{lstlisting.630}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.3}{\ignorespaces Training SimCSE with Triplet SNLI Data}}{50}{lstlisting.630}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.5}SNLI Dataset Preparation}{51}{subsection.661}\protected@file@percent }
\newlabel{lst:snli_dataset}{{B.4}{51}{SNLI Dataset Classes}{lstlisting.662}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.4}{\ignorespaces SNLI Dataset Classes}}{51}{lstlisting.662}\protected@file@percent }
\citation{reimers2019sentence}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}SBERT Implementation for Paraphrase Detection and Semantic Similarity (Hussnain Amjad)}{55}{appendix.785}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{AppendixC}{{C}{55}{SBERT Implementation for Paraphrase Detection and Semantic Similarity (Hussnain Amjad)}{appendix.785}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Overview}{55}{section.786}\protected@file@percent }
\@writefile{toc}{\vspace  {2em}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{devlin2018bert}{{1}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{gao2021simcse}{{2}{2021}{{Gao et~al.}}{{Gao, Yao, and Chen}}}
\bibcite{jiang2019smart}{{3}{2019}{{Jiang et~al.}}{{Jiang, He, Chen, Wang, Song, and Zemel}}}
\bibcite{amahankali2024}{{4}{2024}{{Mahankali et~al.}}{{}}}
\bibcite{mikolov2013efficient}{{5}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{paperswithcode}{{6}{2024}{{Papers with Code}}{{}}}
\bibcite{pennington2014glove}{{7}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{rajpurkar2016squad}{{8}{2016}{{Rajpurkar et~al.}}{{Rajpurkar, Zhang, Lopyrev, and Liang}}}
\bibcite{reimers2019sentence}{{9}{2019}{{Reimers and Gurevych}}{{}}}
\bibcite{cs224n2024}{{10}{2024}{{Stanford University}}{{}}}
\bibcite{vaswani2017attention}{{11}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\@writefile{toc}{\contentsline {chapter}{References}{56}{dummy.787}\protected@file@percent }
\newlabel{References}{{7}{56}{Overview}{dummy.787}{}}
\gdef \@abspage@last{71}
