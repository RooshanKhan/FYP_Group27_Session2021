\documentclass{article}
\usepackage[top=0.1in, bottom=0.5in, left=0.5in, right=0.5in]{geometry}
% \usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\begin{document}
\section{Introduction}

\section{Tasks}
\subsection{Week 1}
\subsection{Rooshan Khan: Attention Method in bert.py}
In this method we had to implement Attention method from class BertSelfAttention. The attention mechanism is given by:
\begin{equation} \label{eqn}
Attention(Q,K,V) = {softmax(\frac{QK^T}{\sqrt{d_k}})V}
\end{equation}
where \( Q \), \( K \), and \( V \) represent the query, key, and value matrices, respectively, and \( d_k \) is the dimension of the key vectors.

I used method \( torch.matmul \) to multiply \( Q \) and transpose of \( K \). I multiplied the result with \( attention\_mask \) to apply the mask. The dimensions of attention\_mask are \( [bs, 1, 1, seq\_len] \). The attention mask distinguishes between non-padding tokens and padding tokens. The non-padding tokens have a value of 0 while padding tokens have a value of a large negative number. The dimensions of key\_layer,query\_layer and value\_layer are \( [bs, num\_attention\_heads, seq\_len, attention\_head\_size] \)

Now I will tell how I concatenated all heads. When we transpose the tensor we change the shape of the tensor from \( [bs,num\_attention\_heads,seq\_len,attention\_head\_size] \) to \( [bs,seq\_len,num\_attention\_heads,attention\_head\_size] \). This enables us to reshape the tensor to \( [bs,seq\_len,num\_attention\_heads*attention\_head\_size] \). After applying the transpose method the data sequence does not follow a contiguous order so we need to use \textbf{contiguous} method before using the \textbf{view}\ method.

\end{document}